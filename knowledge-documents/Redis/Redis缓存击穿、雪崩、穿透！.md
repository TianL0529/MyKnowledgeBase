



















#           Redis缓存击穿、雪崩、穿透































# Redis缓存击穿、雪崩、穿透！

### 缓存击穿

某一个热点数据，缓存中某一时刻失效了，因而大量并发请求打到数据库上，就像被击穿了一样。

那么，缓存击穿，就会使得因为这一个热点数据，将大量并发请求打击到数据库上，从而导致数据库被打垮。

要解决这个问题之前，我们就先得对整个系统的架构有一定的了解：

首先，在客户端很多的情况下，他们必然会去访问我们的 redis；
这时候 redis 是做缓存用的，所以在后面，必然有一个 DB，比如我们的 MySQL；
如果站在外围的层面来说，这个客户端其实就是一个 service；
再往前延伸的话还有很多其它的服务，这个服务只是微服务群体中的一个；
微服务如果再往前，就是网关，可以是业务网关，比如 Springcloud 的 Zuul；
再往前，必定要有流量分发层，负载均衡器等等，比如 Nginx、LVS；
如果项目够大，也可能会有 CDN 这样的把各地的流量分离；
真正在最前面的，才是我们的用户，去访问我们的服务。
真正的流量，也就来自与用户，他们才是行为的主体。
用户的量足够大，才会有所谓的高并发，
我们的系统，也就是从前往后，一层一层的过滤掉各种各样的请求，
真正能抵达我们的数据库的，只有很少的一部分请求，这才是一个架构师在横看一个项目时，应该做的事情。

![202004212025142](D:\Desktop\知识整理\图片\202004212025142.png)
而我们的 redis，在整个系统体系中，作为缓存，就是将很多的请求抗住，过滤掉，
所以最后的数据库的压力就会很小。

那么既然 redis 是作为缓存，

那要么就会给 key 设置过期时间，在一段时间后清除；
或者，就是 LRU、LFU，清除冷数据。
所以说，只要是作为缓存，那么就一定存在这种情况：
一个 key，某一时间，要么过期了，要么 LRU 清除，然后，就突然有人来访问它。
就好像是在 redis 上打了一个窟窿，击穿了，穿过去了。

本来一个系统一个架构，请那么多人，花那么多力气，就是为了能让系统抗住更高的并发，让更少的请求打进我们的数据库里，
结果，就因为一个 key 过期了，这么一个小小的事情，导致所有的请求疯狂打进我们的数据库。

那么这件事情该怎么规避，首先不要去看网上什么到处都是的博客里边的描述，你需要先承认一点，就是肯定发生了高并发。
如果一个系统本来就没啥并发量，那就压根没什么事，请求打到数据库上来呗，完全不是什么问题。

![20200421202458814](D:\Desktop\知识整理\图片\20200421202458814.png)
那么在高并发的情况下，一个 key 过期了，然后，就是几千几万的并发蜂拥而至，这该怎么解决？

有些学艺不精的会给出这些布隆啊、过期时间散开啊、改进缓存算法、延长过期时间这些个答案，
那说明对缓存这个概念还没有深刻的认识。

首先，大部分人会想到这么一个答案：**热点数据永不过期**。在网络上盛行的解决方案有很多：

* 设置 key 永不过期，在修改数据库时，同时更新缓存；
* 后台起一个定时任务，每隔一段时间，再 key 快要失效的时候，提前将 key 刷新为最新数据；
* 每次获取 key 都检查，key 还有多久过期，如果快过期，则更新这个 key；
* 分级缓存，一级缓存失效，还有二级缓存垫背。

这一类的回答还有很多，统称归纳起来，就是**让 redis 的缓存不过期**，普遍的做法就是：

* 设置 key 永远不会过期；
* 在缓存还没有失效前，就更新缓存。

如果你给出过这样的答案，那说明你应该没有在实际的生产环境中，没有真正碰到过，且需要处理这样的情况，
因为确实，存在缓存击穿问题的，光并发量的要求，就可以排除掉 99% 的企业，
所以，真正有实际场景的，去考虑解决缓存击穿问题的人，少之又少。

所以，大部分人，都只是停留于纸上谈兵的阶段，正确与否，没有实际场景的验证，只能靠直觉去判断；
或者，浏览了很多的文章博客，发现人人都这么说，便也自然而然认为，事实就是这么一回事。

下面我来详细分析，为什么，这些解决方案，在实际的生产环境中，是无法胜任的。

首先，我来分析，key 永不失效的解决方案，为什么不可行。

因为，对于一个需要解决缓存击穿问题的企业，他们的业务量一定是普通人无法想象和企及的；
所以，他们的数据量是巨大的，因而需要缓存，去保留热点数据，减轻数据库的压力；
所以根据这一点就可以明确，不存在缓存不会失效的情况。

为什么呢？
因为数据的量巨大，我们的 redis 缓存，是基于内存的，一个单点，一般也不会分配过大的内存，来保证它足够灵活。
但是，即便是集群，所能存储的数据量也是有限的。redis 不可能把所有的数据全部缓存入内存，没有什么企业可以说用内存就可以存储所有的数据。

但是，你可以说，只存热点数据啊！

但是，什么叫热点数据？你觉得是就是吗？

真正的环境中，热点数据是在时时变化着的，我们可以对一些热点做一些预估，但是，我们永远无法保证我们能预估到多少。
在这样千变万化的环境中，一个明星干了点什么事，就能掀起你无法想象的流量；
比如 xxx 怎么怎么的了，然后新浪就瘫痪了，也不是没有的事。
所以，数据是流动的。
所以在真实场景中的热点数据，是绝对不可能是由人去评估的，所有的热点数据，都是根据时时的流量，系统缓存自动过期掉那一部分已经冷门的数据，然后又缓存起新的热点数据。
所以说，热点数据，一定是时时出现，时时消失的，我们靠人的大脑，是无法直接判断出所有在某个时间点会出现的热点数据。
所以这必须由我们的系统，能够直接去应对数据的变化，在巨量数据的流动中找到平衡。
所以，我们的 redis 缓存，也不可能让 key 永远不会过期。
所以，redis 也不是你想让它存一些不过期的数据就行的，由于热点数据的不断变动，redis 必须在时刻淘汰旧的数据，缓存入新的数据。

第二个，网络上很流行的答案就是：加锁

synchronized 加锁，而且还衍生出双重检查锁；
ReentrantLock.tryLock()，缓存没有，尝试加锁，抢不到就睡一会，抢到的那一个查数据库；
redis 的命令 setnx()，只有一个线程能设置成功，也就是能加到锁，只有加到锁了，才读数据库，然后存会 redis 里，其它则等待一会，然后再去 redis 取。
其实加锁确实是可以解决的，
但是，如果你要是写了 synchronized，那你一定会被直接炒鱿鱼。
这种都是严重的问题，会使你的系统可能就瘫了。

首先，对于缓存穿透的情况，肯定是高并发场景，所以数据库才可能扛不住。
所以，查询 redis，或者 mysql 的，一定不可能是单台 tomcat 进程。

所以，在如此多的 Tomcat 集群的情况下，一把 Java 锁，是不可能锁住一个集群的。
而且，synchronized 一但加锁，是不可撤销的，它不像 ReentrantLock 那样，可以 tryLock，加不到锁也可以返回。
所以，一但使用了 synchronized 加锁，会使得所有的读 redis 缓存也加锁。
读请求加互斥锁绝对是致命的，这个系统绝对是一启动就被流量击垮。

虽然说，用 ReentrantLock，tryLock 加锁，成功的去数据库读取数据；
而那些失败的，则睡眠一段时间，再重新去缓存读取，
这个流程已经开始像实际的解决方案了。

但是，一个 Java 锁最多只能够锁一个 JVM 进程，对于一个集群来说，这绝对是远远不够的。
而且，去 redis 里读取数据的，可能不仅仅只是 Java 进程，像 Nginx 也能直接访问 redis、mysql，是不是有点超出你的认知？
如果你的解决方案仅仅是一把 Java 锁，那么绝对达不到生产环境的要求。

所以实际上，缓存穿透，加锁解决，必须还要涉及到分布式锁的概念。

这里不谈 zookeeper 之类的东西，既然谈 redis，那么就用 redis 来解决这个问题。

首先，当一个 key 失效，不管是时间过期，还是被 LRU、LFU 剔除，
假设会有 1w 个并发来访问这个 key，那么它们就会先查询 redis，然后都发现，这个 key 不存在；
然后，它们就会对应的，往 redis 用 setnx`（SET If Not Exists）` 设置一个 key，来表示这是一把锁；
然后，只有一个线程，会设置成功，然后去读取数据库，写回 redis；
其他的 9999 个线程，则 sleep 一小会，然后再去访问我们的 redis。
有人看到这，首先会问，这个 sleep 要多久？
这个是要根据压测，以及线上环境进行调整的，一般会给出一个合适的值，也就是大约从数据库取出数据的时间。
所以，正常情况是不会出现大面积长时间等待的情况的。

![20200421202444138](D:\Desktop\知识整理\图片\20200421202444138-1625811068597.png)
看起来似乎可行，但是，还有问题吗？

我要这么说肯定是有问题，但是，你可以想一想，存在什么问题？

如果你不知道，说明对分布式锁还不够了解，那么，就继续跟着我分析。

现在，我开始假设：

首先，一堆请求访问 redis，发现为空；
然后，这一堆并发开始尝试加锁，最终只有一个人，获取到了锁，其它人都失败；
然后，**持有锁的机器，断电了**；
其他人，一直等着，但是始终没有人等到锁被释放，或者 redis 被重新存入该数据。
这是一个分布式锁最常见的问题，就是加锁进程死亡，导致锁无法被释放。
于是就产生了死锁问题。

![20200421202428907](D:\Desktop\知识整理\图片\20200421202428907.png)

现在，既然出现了问题，那么，我们一定得想办法去解决。

首先，对于我们的分布式集群系统，任意一台机器都有挂掉的可能。
所以，我们首先要明确的思路就是，如何在加锁进程死亡的情况下，去释放这个锁。

可以想到两种方案：
第一：
就是另起一个集群，负责专门监管锁的获取和释放；
一但持有锁的进程宕机，监管集群就负责将死锁给释放。

明显，这么做成本比较高昂，还不如用完善的 zookeeper 去实现分布式锁。

第二种：
就是平时比较常见的，用 redis 的设置过期时间，来保证，即使宕机，锁也能在超时过后自动释放。

于是，之前的方案，就可以稍作修改：

首先，一堆并发开始尝试加锁，最终只有一个人，获取到了锁，其它人都失败；
然后，**锁被设置上过期时间，保证无论如何一定会被释放；**
然后，持有锁的机器，断电挂了。。。；
其他人，等了一会，发现锁又没了，于是重新开始之前的操作。

![20200421202416115](D:\Desktop\知识整理\图片\20200421202416115.png)

看起来很完美。

但是，还有问题吗？

我要这么问了，那么一定说明有。

那么，现在请你先不要拖到后文，先自己思考，会存在什么问题，然后再来看我的分析。

现在我继续列出场景，
假设：

首先，一堆并发开始尝试加锁，最终只有一个人，获取到了锁，其它人都失败；
然后，**持有锁的人还没来得及设置锁的过期时间，就挂了**。。。
其他人，一直等着，但是始终没有人等到锁被释放，或者 redis 被重新存入该数据，又是死锁。
既然问题来了，我们就需要想办法，去加以解决。

首先，可以确定的是，可能锁没有来得及增加过期时间，从而导致，可能出现死锁的情况。
因为，之前的**设置锁、和设置过期时间，是两步操作，不是原子的**。

有些人可能就会说，那就放一个原子操作啊！

但是，redis 并没有一个 API，既可以 setnx，又同时给予它一个过期时间。**(set指令是可以把这两个操作合在一起)**

那该怎么办？

所以，这就需要考验，我们对 redis 的各种机制的掌握程度了。

首先，redis 有事务这么一个概念，
不过，redis 的事务不像 mysql 那样，可以支持回滚。

那么不能回滚的事务也可以用来完成锁操作吗？

虽然不支持回滚，但是主要是因为 redis 的事务是保证原子性的：
事务中的命令要么全部被执行，要么全部都不执行。
如果客户端在使用 MULTI 开启了一个事务之后，却因为断线而没有成功执行 EXEC ，那么事务中的所有命令都不会被执行。
另一方面，如果客户端成功在开启事务之后执行 EXEC ，那么事务中的所有命令都会被执行。

在一个事务中，只有全部的命令发送结束了，并且提交事务，那么整个事务中的所有指令，才会被 redis 执行；
也就是说，在尝试去给 redis 的一个 key，加锁，只要不最终 EXEC 触发事务，那么这些方法就永远不会被执行；
那也就是，要是 EXEC 触发事务执行，就一定会执行加锁和设置过期时间的命令。
否则，没有 EXEC，就两条指令都不会执行。

这样，就可以保证，redis 不会出现死锁的问题。

这样，解决了死锁问题，就看起来很完美了。

但是，
这样就可以了吗？

确实，如果只是解决了死锁的问题的情况下，是没有什么问题的。

但是，因为我们在解决死锁问题的时候，引入了超时时间，所以，就会导致新的问题的产生。
我们在解决一个问题的时候，往往会引入新的问题。

现在，假设：

首先，一堆并发开始尝试加锁，最终只有一个人，获取到了锁，且设置了超时时间；
然后，持有锁的线程，开始进行读数据库的操作；
但是，由于各种不确定因素，它这次读数据库读得很慢，所以还没结束，锁就超时释放了；
然后，第二个线程也拿了一个锁，开始它的操作；
然后，第一个线程结束了，这时，本该所有其他线程可以访问数据库了，但是，由于第二个线程去加了锁，导致现在它们得额外继续等到第二个线程去释放；
这样，就会增加了等待时间，响应延迟就会增大，如果，再多几次加锁过程，响应延迟就会越发严重，或者直接超时断开。
![20200421202352132](D:\Desktop\知识整理\图片\20200421202352132-1625812619928.png)
所以，在设置锁的超时时间的时候，该怎么设置？
设置短一点？
那就会很容易发生锁被别人又抢过去的情况；
那设置长一点？
那么就又可能使得阻塞时间变长。

所以，锁的超时时间又成了问题。

既然新问题出现了，我们就得想办法去解决它。

而现在普遍的解决方案，就是多线程：

在加锁了之后，由于锁会有过期时间，然而又不能保证，锁一定不会在执行结束过后过期，
那么，我们就可以**采用多线程的方案，让锁每隔一定时间，就重新设置它的超时时间**。

于是就出现下面这样的场景：

首先，一个手速快的家伙抢到了锁，并且也设置了超时时间，比如 30 秒。
然后，一个线程执行业务操作；又另起一个线程，去监管锁的时间；
假设，这个业务做起来比较漫长，过了 10 秒还没结束；
于是，监控线程感觉不妙，于是将过期时间又重新设置成了 30 秒；
业务继续执行着，然后又过了 10 秒，锁的过期时间还剩 20 秒；
于是，监控线程又感觉不妙，于是将过期时间再一次重新设置成了 30 秒；
周而复始，只要业务没做完，锁就不会过期；
假设 1，进程挂了，然后，30 秒一到，锁被释放；
假设 2，业务执行完了，于是线程主动释放锁。
于是，多线程的技术，就把这个缓存穿透的方案给解决了。

![20200421202338489](D:\Desktop\知识整理\图片\20200421202338489-1625812741983.png)


是不是觉得巨麻烦，竟然要从头到尾经历这么多的过程，才能最终，实现一个不起眼的缓存穿透！

不过，实际上你再细想，其实，我上面提及的各种问题和解决方案，都刻意回避了一个问题：
就是，redis 是单节点单实例的。

也就是说，我们对这一个 key 的操作，都是在一个 redis 上，而没有同时牵扯到其他 redis；
所以，只要这个 redis 不挂，那么就不存在问题。

不过要是 redis 挂了，那么面临的问题，也就不是 redis 的缓存击穿问题了。
而是系统的高可用的解决方案，比如我上一篇文章提到的：
redis 的主从、主备，哨兵监控，来保证 redis 挂了之后，能立刻有 redis 前来替补。

文章指引>> https://blog.csdn.net/weixin_44051223/article/details/105578206

因为后面的这些个知识点，对集群有相关的知识，所以，我也很建议，你可以看一下我的这一篇文章。
你也可以先看后面的，然后看完后，去看我之前的这一篇文章，做一次知识的整合和理解，然后再回过头来看到这里，也许你又会有不一样的感觉。

不过，即使是主从模型，允许 redis 的从节点也提供读服务，
这样就会存在数据在一定时间内不一致的情况，那么其实也没有太大的问题。

假设：

第一个线程，抢到了锁，然后访问完数据库，将数据写回主结点；
然后，其它线程去从结点读取，由于可能数据同步的不及时，导致一部分结点读出的数据还是空；
于是，那些读同步不及时的那部分从节点的线程，再重复一遍之前的操作；
这样，就可能出现部分线程的往复多次操作，一直读，一直是空。
如果从节点多的话，那么所有的从节点之中，至少大部分从结点，都是通信正常的，
一般不会出现大面积坏死的情况；
所以，如果有少量从节点没有数据，那么会导致的二次重新操作，也只是少量的一部分线程，这样，也只是再次加锁一次，多读取了一次数据库。

然而实际上，也根本并不用那么麻烦，假设**从节点没有读取到，可以直接去主节点读取**，那么就不会出现数据迟迟读取不到的情况了。
也就是，对于这样的加锁操作，没有必要要去涉及到从节点，所有的锁操作，直接对主节点即可。

所以说，通过双线程的加锁操作，是可以解决缓存击穿的问题的。

不过，由于我在上文，提到了这是一个分布式锁的概念，
要是，我在这里，仅仅就这么结束的话，难免会有同志误以为这就已经是一个完美的分布式锁，
所以，我再稍微提一下，redis 集群的分布式锁的知识点。

对于单个 redis 来说，上面的知识点已经可以实现分布式锁了。

但是，既然要讨论高并发高可用的系统，就会涉及到集群。

对于单个 redis 来说，假设，加锁的 redis 挂了，那该怎么办？

redis 的主从模型，默认使用异步同步数据的方式，所以，存在数据不一致的情况，
主节点挂了，从节点顶替的时候，是可能丢失数据的，

所以，这把锁很可能就丢了。

为了能够解决这样的问题，Redis 的作者 antirez 给出了一个更好的实现，称为 **Redlock**，算是 Redis 官方对于实现分布式锁的指导规范。

如果你们不善于阅读英文，那么就直接看我中文的描述：

在算法的分布式版本中，我们假设有 N 个 redis，且这些节点是完全独立的，也就是不存在任何主从关系，一个 redis 的死活和其他 redis 没有任何关系。

那么，接下来，就请思考一下，加锁的操作：
首先，既然是**分布式锁**，那么就不能只对单台结点加锁，因为上面已经描述过了，一但该结点宕机，就可能会使得锁丢失，因此，存在单点故障的问题。
所以，就必须对集群中的多个结点加锁。

那么，应该给几台结点加锁呢？

如果采用全部结点加锁成功，才表示加锁成功，那么就成了**强一致性**，
如果你阅读过我的《Redis从单点到集群》这篇文章，应该能明白，强一致性，会对可用性产生冲击，因而不适合采取这样的方式。

那么应该给几个结点加锁成功，才表示加锁成功呢？
在 Redlock 的实现中，加锁的 redis 结点，只要满足，**N/2+1，也就是过半，即代表加锁成功**。
为什么要过半呢？
因为**过半才能保证，真正加锁成功的，只有一个**。
过半又不要求全部，这样，保证了持有锁的唯一性，并且也保证了集群的可用性足够好。

那么既然最基础的问题解决了，下面，假设出现这么一个场景：
假设，N=5，有 5 台 redis：

一个线程，向 redis 集群发起加锁操作，然后第一个结点加锁成功了；
然后，它又紧接着立刻向下一个结点发起加锁，也加锁成功了；
然后，它又向第三台 redis 加锁，也加锁成功了；
那么，这时候，已经代表，它获得了 redis 集群的分布式锁；
但是，它不知道，它前两个节点加的锁，已经过期了，这时候，它只加了一把锁。
然后，另一个人揭竿而起，也立刻加上了 3 个节点，也代表获取了锁。
这下，就出现了一个集群，有两人持锁，锁就不可靠了。
不过，在之前提到过，我们可以给 redis 的锁，延续超时时间。
于是，假设：

一个线程，向 redis 集群发起加锁操作，向 1、2 redis 加锁，都加锁成功了；
但是，加锁到 redis 3，由于网络通信延迟，一直卡在那加锁；
这时，另一个哥么看不下去了，于是他也发起加锁操作；
于是，它向 redis 1、2 开始加锁；
1、2 因为已经被加过锁了，所以加锁失败，然后加锁 3、4、5；
但是，由于它和 4、5 连接有故障，导致无法加锁成功；
然后，这时第一个加锁的哥们，由于网络故障，也没有加锁成功；
从而，俩人都没加锁成功。
其实，如果没有第一个家伙，第二个哥们是能加到锁的，
但是，由于第一个加锁者，占据了锁的位置，占用了大量的时间，导致之后加锁的线程，就会因为被占用，很容易加不到锁，就会使得加锁资源被白白浪费，系统的加锁过程就会变长，效率变低。

所以，为了解决这个问题，就**可以设置一个超时时间的概念，让加锁的每一步，都快速，轻盈**，
加的到就加的到，加不到就加不到，过程迅速，不拖泥带水，
这样，就能使得加锁的过程更迅速，加锁冲撞而导致加不到锁的概率也会变低，从而使得加锁更加高效。

所以，加锁的时候，设置超时时间，但是，如果加锁最终没有成功，就不给单独结点上的锁续命，就让它快速过期，这样，就能够使得集群之间的加锁更加高效迅速，而不容易出现争抢激烈的情况。
所以，在这里，就不应该像之前那样，给锁延长超时时间。

所以，在整个加锁过程中，整个加锁的过程，不能超过锁的有效时间，否则，就应算作加锁失败，要立刻清除所有单独结点上的锁。

![20200421202321868](D:\Desktop\知识整理\图片\20200421202321868-1625813882676.png)
现在，想来你应该能大致理解，Redlock 加锁的大致过程了，下面我就用简略的语言，翻译一下官方对于 Redlock 的加锁操作：

首先获取当前时间（毫秒数）；
按顺序依次向 N 个 redis 节点获取锁，其中要保证 key 相同，且 value 随机；
为了保证在某个 redis 节点不可用的时候算法能够继续运行，获取锁的**操作还有一个超时时间，它要远小于锁的有效时间（几十毫秒量级）**。
客户端在向某个 redis 节点获取锁失败以后，应该立即尝试下一个 redis 节点。这里的失败，应该包含任何类型的失败，比如该 redis 节点不可用，或者该 redis 节点上的锁已经被其它客户端持有（注：Redlock 原文中这里只提到了 redis 节点不可用的情况，但也应该包含其它的失败情况）。
计算整个获取锁的过程总共消耗了多长时间，就是用当前时间减去第1步记录的时间。
如果客户端从大多数 redis 节点（>= N/2+1，也就是过半）成功获取到了锁，**并且获取锁总共消耗的时间没有超过锁的有效时间，那么这时客户端才认为最终获取锁成功，否则，认为最终获取锁失败。**
如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第 3 步计算出来的获取锁消耗的时间。
如果最终获取锁失败了（可能由于获取到锁的 redis 节点个数少于 N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么**客户端应该立即向所有 redis 节点释放锁。**
加锁的过程比较复杂，不过释放锁的过程就简单多了：
向所有 redis 节点发起释放锁即可，不管这些节点当时在加锁的时候成功与否。

看起来似乎很完美了，但是，我继续抛出一个问题。

假设一共有 5 个 redis ，分别是 ABCDE：

客户端1成功锁住了A, B, C， 加锁成功（但 D 和 E 没有锁住）。
节点 C 崩溃重启了，但客户端 1 在 C 上加的锁没有持久化下来，丢失了。
节点 C 重启后，客户端 2 锁住了 C，D，E，加锁成功。
这样，客户端 1 和客户端 2 就同时获得了锁。
![20200421202304100](D:\Desktop\知识整理\图片\20200421202304100.png)
这时候该怎么办？
我们是不能保证，在分布式及群中，没有结点会宕机的。

在默认情况下，redis 的 AOF 持久化方式是每秒写一次磁盘（即执行 fsync），因此最坏情况下可能丢失 1 秒的数据。
为了尽可能不丢数据，redis 也允许设置成每次修改数据都进行 fsync，但这会降低性能。
当然，即使执行了 fsync 也仍然有可能丢失数据（这取决于系统而不是 redis 的实现）。
所以，上面分析的由于节点重启引发的锁失效问题，总是有可能出现的。

那么，既然锁可能因宕机而丢失，已经无法再恢复。于是，antirez 又提出了**延迟重启 （delayed restarts）**的概念。
也就是说，一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，**这段时间应该大于锁的有效时间**。
这样的话，只要这个结点不重启，如果此时，持有锁的线程所占据的 redis 只剩下了 2 台，那么，这把锁就无法被继续维持，那么，只要失效时间一到，锁就会被保证被迫过期释放。
所以，延迟重启，就使得这个节点在重启前所参与的锁都会过期，它在重启后就不会对现有的锁造成影响。

不过关于 Redlock 还有一点细节值得拿出来分析一下：
在最后释放锁 的时候，antirez 在算法描述中特别强调，客户端应该向所有 redis 节点释放锁。
也就是说，**即使当时向某个节点获取锁没有成功，在释放锁的时候也不应该漏掉这个节点**。

这是为什么呢？

设想这样一种情况，客户端发给某个 redis 节点的获取锁的请求成功到达了该 redis 节点，这个节点也成功执行了 SET操作，但是它**返回给客户端的响应包却丢失了**。
这在客户端看来，获取锁的请求由于超时而失败了，但在 redis 这边看来，加锁已经成功了。
因此，释放锁的时候，客户端也应该对当时获取锁失败的那些 redis 节点同样发起请求。

因为这种情况在异步通信模型中是有可能发生的：客户端向服务器通信是正常的，但反方向却是有问题的。

谈到这里，对于缓存的击穿，以及涉及到的一点 redis 分布式锁的知识，你应该已经了解得差不多了。
而且，其实实际上，你会发现，我谈论到的知识，绝对是不限于仅仅在这个 redis 身上的，而是这整个架构的设计和思维。
如果你仅仅把眼光放在这个小小的 redis 上，那你是永远不会达到架构师的水平的。

**缓存雪崩**
其实把缓存击穿搞清楚了，那么你去理解缓存雪崩也会容易许多。

缓存雪崩，指的是大面积的 key 同时过期，导致大量并发打到我们的数据库。
不像击穿，只是因为 1 个 key 的过期。

所以，对于雪崩来说，一般，少量的 key 失效，所带来的数据库的并发压力是不会太大的。
而是大量 key 的同时失效，导致所有 key 的并发加起来，会影响到我们的数据库。

那就算一个 key 失效，也会对数据库造成很大的影响，那么你把雪崩的所有 key 拆成一个一个 key 来看，也就是雪崩可以拆分成一个一个缓存击穿的集合。

其实在真实场景中，雪崩才是一个更容易发生的一个问题，它不像击穿那么极端，一个 key 就成千上万的并发，直接把数据库打垮了；
而是，可能就一个 key 几十几百的并发，然后大量的 key 一过期，然后就使得好多并发，同时叠加起来，累积到上千上万个，把数据库打崩了。

那么既然缓存击穿已经给过解决方案了，那么我们现在要关注的，则是如何缓解雪崩所带来的压力。

因为，key 是同时失效，所以导致很多 key 的并发，一起压上来，才会使得数据库的并发压力过大，
所以，我们很正常的思路就是，就是**让并发分散开来**。

首先一个很常见的做法就是，**分散 key 的过期时间**。

确实，这么做是可行的，因为这个问题的本质，就是要让瞬间到来的并发，把它分散开。
而给了一定的随机过期时间之后，就能够使得 key 会分散开，一个一个过期，
所以，并发量就会分成一部分，一部分，少量的打到数据库上。

看起来就很像一个**削峰**的操作。

这个方法，是最简单有效的。所以一般情况，我们都采用这种方式。

![20200421202247221](D:\Desktop\知识整理\图片\20200421202247221.png)
不过，要考虑一种情况，就是，如果你的业务对**时点性**要求高，必须每天的指定时间，去更新我们的数据。
就比如游戏每日零点更新，或者财报记录……等等等等。

就是，在某一个固定的时间，由于业务要求，必须使得数据刷新，并且不允许出现旧数据。
所以，必须让缓存全部失效。

像这样的业务应该怎么办？

因为这个场景，非常类似削峰操作，所以有人会觉得，可以用 MQ，先把读请求打入 MQ，再一个一个依次消费。
![2020042120223146](D:\Desktop\知识整理\图片\2020042120223146.png)

这样可行吗？

首先，从系统实现来说，是可以保证，数据库的请求压力先被扛下来，然后异步消费。
但是，对于读请求，是不应该用到消息队列的。
如果是异步写，没有什么问题，用户只要求能把数据存入即可。
但是对于读，如果用队列依次读取，那么，大量用户的响应延迟，就会变高，这对用户体验的影响是不容忽视的。

所以，对于读请求，不适合用队列的方式，因为这已经把请求串行化了，不再是并发执行。

于是，还有人提出观点，让缓存提前开始更新。

但是，提前更新了之后，比如 58 分开始更新，59 分的时候，有大量数据又被修改了呢？
所以，数据是不准确的，那些 59 分修改了之后的客户，在 12 点查看数据的时候，发现数据仍然没有变化，他们就会认为，系统的 12 点更新的说法不靠谱，公司不值得信任。

所以，缓存是必须要在 12 点准时失效，准时更新的。你无法让更新时间进行变化。

那么，你还能想到什么办法？

因为此刻，redis 中的数据，是必须立即失效的，你不能够改变。
那么，对于 redis，就不能够把时间分散开来。

既然 redis 不可以，那么其它地方可以吗？

所以，这就是考验你思维和功力的时候。

既然 redis 无法分散过期时间，那么，我们去查数据的时候，是不是可以把时间稍微地分散一下？

所以到了下面这种情景：

时间一到，redis 数据全部失效；
大量并发前来查询；
在 service 服务层，查询时，**设置一个短暂的随机延迟时间**；
这样，于是查询的操作，就被分散了开来；
少量的请求，先查询，就会读数据库，然后存入 redis；
其他请求，由于随机时间，稍稍慢了点，就可以去 redis 读出数据。

![2020042120221197](D:\Desktop\知识整理\图片\2020042120221197.png)
这就是，从业务层，再把时间分散。

带来的影响，也就是客户等待时，会多那么几十毫秒的延迟，不过对于人来说，是微乎其微，可以接受的。

所以，对于时点性要求高的业务要求，雪崩的问题，想要解决，还必须稍微多思考，变通一下。

#### **缓存穿透**

很多人会把缓存穿透和击穿搞混，主要是名词方面的混淆。
更多的，我觉得关注意思即可。

缓存穿透，与击穿的区别就是，
**击穿：数据库里“有”数据；**
**穿透：数据库里“没”数据。**

所以，缓存击穿可以规避，因为只是 redis 缓存数据失效了，而数据库里有数据，只要把数据库里的数据更新到 redis 上，那么就可以解决掉缓存击穿的问题。

但是，缓存穿透，意味着，这个数据，数据库里也没有。
所以，就不可能会把数据存到 redis 缓存里，因此只要有人来查询，就一定缓存中查不到，所以就一定要走数据库。
那么，假设很多人，故意去查那些数据库里也没有的记录，我们的 redis 就起不到屏障的作用，因为 redis 里不可能有数据，所以并发查询就一定会打到数据库的身上。

那么，想要解决缓存穿透，就必须想办法，能够识别出，哪些请求的数据，是数据库没有的，然后，对这些请求的查询，进行过滤。

如果你以前没有了解过这些知识，那你可以先想一想，可以用什么办法？

比较简单的，就是选择，当用户查询不存在的数据时，将这个 key，存入 redis，然后用一个特殊的 value 来表示，这是一个不存在的数据。
但是，如果有大量的请求，都请求各不相同的不存在的数据，那么，redis 的缓存，就会用来存储大量没用的数据，就会造成空间的浪费。
而且，一般这部分的请求，都是人刻意攻击服务器。

而且，很明确的一点，就是数据是无限的，我们不可能找出所有的数据库中不存在的数据；
但是，对于数据库里已有的数据，那就是有限的，所以我们可以找出所有已经存在的数据；
这样，当请求打过来的时候，我们就能以此来判断，这个数据是否存在。

但是，由于数据量的巨大，我们必须得想一个方法，怎样用尽可能少的空间和时间，去对数据是已有在做一个判断。
正是因为 redis 的小，内存空间的可贵，才使得，我们不能够去缓存所有的数据，因而才会有查询 DB 的操作。
所以，我们不可能直接将完整的数据信息全部搬入内存。

那么，既然数据无法完整存储，那么是否可以，只保留 key，省略 value，从而使得单位范围内内存能存储的信息量大了很多。
因为数据的大部分空间，都是 value 占用的，一般 key 和 value 相比，都是非常非常小的。

所以，是不是就可以额外开辟一片 Set，用来专门存储 key，这样，每次要访问数据库前，先去 key set 中查询是否存在，如果存在，那么再去访问数据库。

这样，确实可以使得缓存不会穿透了。而且相比缓存全量 key、value，只存储 key 会使得内存的占用变小了很多。

但是，理论上听起来似乎不错，假设，我们一台 redis，用来缓存后端 4T 的热点数据；
现在为了实现缓存穿透的解决，假设一个 key : value = 1 : 99，那么需要缓存的总量 key 大约要 40G。

所以，到了数据面前，就能发现仍然是一个超高成本的方法。

那么，既然如果存储 key，空间仍然很大，那么我们能否想出一个更节省空间的存储方式？

一般有点经验的都会想到用 bit，也就是用一个位来存储，这已经是计算机中的一个最小的存储单位了。

现在，先不管如何实现，我们先来看一下，假设用 **bitmap**，那么空间的花费代价有多少。
因为一个 key 只占用一个 bit，所以，假设我们花费 1 个字节的空间，就能存储 8 个 key；
假设，我们用了 1 M字节的空间，就可以存储 800w 个 key；
那么我们在增多一点，用 100MB，那么就能存储 8亿 个 key；

由此可见，用 bitmap，确确实实可以达到对空间利用的极致。

那么，既然空间的问题解决了，下面就要解决如何使用这些空间：
也就是如何把一个 key，和一个 bit 去对应起来。

有点经验的，立刻就能够想到，用哈希映射。
就比如我们的 HashMap，我们的每一个 key，都能通过哈希函数，转换成一个数组的下标，然后将键值对，存储在 HashMap 中。
HashSet，就是只存储 key，而不存储 value。

所以，采用哈希映射，就可以将那些所有存在的 key，全部对应到这个 bitmap 的每一个槽位上，
这样，我们可以将所有存在的 key，把它映射到一个 bit 槽位上，然后用 1 表示，其他剩余的部分，就用 0 表示，
这样，当一个查询的 key 被映射到 0 这个槽位，那么就代表这个数据不存在，所以就可以直接返回。
因此，就可以实现对请求数据的过滤。

看起来似乎很完美，既解决了空间的问题，又可以保证每一个 key，能够映射到一个槽位上。

但是，仔细一思考，就会发现，其实还有问题。

首先，我们先谈我们熟悉的：HashMap。
我们知道，如果 HashMap 出现冲突的话，会用一个链表去连接起那些冲突的结点，从而保证，所有的 key，都能在里面完好无缺的存着，不存在不同的 key 将其他 key 挤占的情况。

但是，由于此时的槽位已经缩减为 bit，已经不能够再往上去追加其它的数据结构了，所以，就无法用链表解决冲突碰撞的产生。
而且，不单单是不能用链表，还有一个原因，就是由于 bit 槽位只存 0、1，即使碰撞，也无法判断原有的信息是什么，到底是不是同一条信息。

所以，bitmap 由于它的精简，因此，不能够将碰撞给消除解决。
![20200421202150321](D:\Desktop\知识整理\图片\20200421202150321.png)

那么，该怎么办？

实际上，这个问题是无法被完全解决的，由于节省空间，每一个槽位被精简到了一个比特，所以，能表示的信息已经只有 0、1 两种，从而无法表示出其他信息。
除非增加空间来保存更多的信息，否则就无法被解决。

那么，假设增加空间，加一个 bit，体积就会增大一倍；再加两个，体积又会翻一倍；
看起来，这种方式，为了绝对的解决冲突，花费的代价，是有点高的。
我们费尽心思节下来的空间，又会被重新花费。

那么，既然作为软件工程的学习者，我们必须有这么一个思维，**就是不较真，不去追求极限**。
比如**强一致性**，因为**会破坏可用性**，所以**一般都会采取弱一致性，或者最终一致性**。

这里也是如此，既然不能解决冲突问题，那么，可以想办法，让冲突发生的概率更小，而不是去完全地让冲突消失。

所以，就可以延伸到**布隆算法**。

首先，因为在单次哈希的情况下，会产生一定的碰撞；
因此，为了降低碰撞而导致的概率，于是，采用多次哈希的方式，每一次哈希都往一个槽位写上 1；
当来查询一个不存在的 key 的时候，就可以进行同样的多次哈希，第一次可能碰巧撞对，得到 1，但是后面还有两次，这样，就不一定有那么好的运气，还能够撞对。
因此，这样，可以降低缓存穿透的概率。

这样，只要对应出我们的需求，去调整 bitmap 的大小，以及哈希函数的个数，就可以得到不同的过滤的百分比，虽然可能出现漏网之鱼，不过那也已经是少之又少了。

![20200421202128582](D:\Desktop\知识整理\图片\20200421202128582.png)
不过，对于布隆过滤器，我们的使用还是需要去思考一下的。

首先，对于布隆过滤器，我们可以把它放在客户端。
也就是每一个客户端都包含这么一个算法，以及存储了一个 bitmap，来过滤无效的查询请求。
这样，redis 的压力就比较轻。

不过，由于是基于JVM内存的一种布隆过滤器，
所以**重启即失效**；
而且由于存储在本地内存中，导致无法应用在分布式场景；
因此也不支持大数据量存储。

第二，我们也可以选择，客户端包含算法，然后，把 bitmap，存到 redis 上去，
这样，客户端就是无状态的，因而可以轻松的复制。

第三，或者，还可以，直接把算法，和 bitmap，一并放到 redis 上去，也就是在 redis 当中集成这么一个模块。
这样，客户端就又省去了代码，就能够更加灵活；
而且也省去了重启失效和定时任务维护的成本；
但是，由于布隆过滤器外迁到了 redis 上，从而会导致网络 I/O 的开销增大，并且性能会比在 JVM 上的 Google 布隆过滤器性能略低。

不管怎样，至少，穿透的问题，似乎已经迎来了大结局。

不过，你有没有想到布隆过滤器有一个缺点，
就是，我们谈到这里，好像只字未提，**布隆过滤器的删除**。
也就是，我们的布隆过滤器，只能往里边添加数据，而不能够删除数据。

你现在想一想，是不是这么一个情况！

也就是说，如果数据频繁增删改，是不太适合用布隆过滤器的。
因为，一个数据变更之后，布隆过滤器无法删除 key，因此，只能重新创建一个布隆过滤器，再加载一遍所有的数据，创建出 bitmap。

那么，解决的话，可以用**布谷鸟**这样的，带删除功能的，来满足动态变化的需求。



# Redis 从单点到集群

#### **AKF代表的三个维度的解决思想：**

* 水平主从复制  --解决 单点故障问题

![读写分离](D:\Desktop\知识整理\图片\20200417140617760.png)

无法解决的问题: 

1.没办法解决容量有限的问题，因为所有的节点都是对主节点的复制；

2.假设业务量继续增大，那么只靠主从复制，虽然可以靠从节点分离读请求，但是主节点的写请求依然无法分离，压力仍然会成为瓶颈。

* 垂直业务拆分  --进一步解决压力问题

![y业务拆分](D:\Desktop\知识整理\图片\20200417142356319.png)

* 逻辑优先级等再拆分

![z轴](D:\Desktop\知识整理\图片\20200417143613224.png)

这样，通过三个维度的集群，来达到对结点的无限扩展，以解决不足。

#### 数据一致性

虽然说上面的 AKF 看起来很完美，不过这终究是一种高层次的划分，没有涉及到各种下层的实现。

为什么一般不能对系统进行过度设计？
最主要的就是，因为往往一个技术点，为了解决某个问题，必然会带来新的问题。

首先，我们先看主从复制，也就是 AKF 的 x 轴。

假设，某一时刻，一个客户端，对主节点发生了写操作，比如 set k1 v1，
然后，主节点需要把自己的数据，同步到从结点。

![主从](D:\Desktop\知识整理\图片\20200417150802815.png)

这就立刻涉及到了一个问题，就是数据一致性问题。

最普通的解决办法就是，客户端发送写请求给主节点之后，主节点阻塞不返回，给后边的从结点写入数据，直到两个从结点都写入完成，主节点才给客户端返回写入成功。

这就是强一致性，让所有结点阻塞，直到数据全部一致。
不过这是所有企业都追求的，但是无法实现且成本及其高昂的一种一致性。

假设有这么一种情况，客户端给主节点写了一个数据，很快完成了，然后在同步从节点的时候，由于网络抖动，丢包等现象，导致某个结点写了很久才写成功。
我们都知道，客户端有一个超时的机制，这么久都不返回，那就直接表示失败了，
就是客户端直接认为这次写操作失败了，换言之，写失败，就代表着服务不可用，

也就是，强一致性，会降低可用性。

![同步阻塞](D:\Desktop\知识整理\图片\20200417154426863.png)


然后，就可以思考一个问题，为什么要把一个 redis，复制出多个 redis。
因为一个 redis 会单点故障，对不对？
所以采用多个 redis 要解决单点故障，也就是可用性问题，对不对？
但是，追求强一致性，反而丧失了可用性，那是不是就有违我们集群的初衷！

所以强一致性是有问题的。
所以，我们就必须对强一致性降级。

那么，我们就可以选择，容忍一部分数据丢失。
我们让 redis 异步往从节点更新数据。
假设，客户端往主节点写了一个数据，然后就直接返回，对于客户端来说，就已经表示写入成功了，
然后，redis 主节点，异步地往从结点去写数据。
但是，假设从结点挂了，或者，网络断了，那么数据就会写失败，从节点的数据就丢了。

所以，用这种异步方式，就需要容忍一部分的数据不一致。

![异步](D:\Desktop\知识整理\图片\20200417154643920.png)

或者，假设我们不希望数据丢失，尽量保证最终一致性，那么还可以对上边的异步模式做一个改进。
于是，可以用一个可靠的消息中间件，各种 mq、kafka 集群等，
写入主节点之后，不直接将数据同步到从节点，而是先存入 kafka 集群，然后，再从 kafka 将数据刷到从节点。
这样，由于 kafka 集群保证可靠，再加上响应的速度足快，
既可以迅速返回响应，又可以保证数据最终一致性。

![最终一致性](D:\Desktop\知识整理\图片\20200417154751829.png)

现在，我们就可以看出，虽然舍弃了强一致性，可以提高系统的可用性，
但是，不保证强一致，就必然会出现这么一种情况：

假设，客户请求了一个随机的 redis 结点，然后，这个时候，由于数据还没有被同步过来，就会取到还没有同步的数据，也就是取到了不一致的数据。
这就是无法避免的一种情况，所以这就需要我们系统架构时的取舍。

#### 主仍然是单点

对于上面提到的，采用主从的这么一种方式，也就是主可读写，从只读。
还有主备模型，客户端只操作主，而不能访问备。

不论是哪一种，都只有一个主，也就是，主，自己就是一个单点，
那么无论有多少从再后面跟着，从挂了没事，
但是，主挂了，立刻就是单点故障。

所以，就涉及到，要对主做高可用（HA）。

高可用不是指主不会出问题，而是指，在主挂掉了之后，立刻会有一个节点来顶替它，
对外的表现是，从来没有出现过问题。

所以，要解决主的单点问题，那么，就可以在主挂掉之后，人工把一个从、或者一个备，去设置为主，让其它节点去追随它。
但是，很多时候，我们都是希望一切是自动化的，
人毕竟手速有限，当发现主挂了之后，要去亲手操作，肯定会花费一定时间，
此外，由于主挂掉的情况，毕竟只是少数，所以大部分的监守都是无效的，只有偶尔，很少的情况，才会主挂掉，找备顶替。
而且人需要吃饭，上洗手间，睡觉，休息，过成人生活…不可能 24 小时一直盯着机器。
所以人工监控的成本是很高的。

那么，我们就需要由机器，来自动帮我们完成这件事。
其实，机器和人也很相像，人会休息、睡觉、生病，所以不能 24 小时可靠；
监控程序也会故障，所以也是不可靠的，只要是一个程序，它就会有单点故障的问题；
因此，监控主节点，也需要多个结点，组成一个集群。

![监控集群](D:\Desktop\知识整理\图片\20200417163043415.png)


但是，用多个节点去监控一个 redis 进程，是否会存在什么问题？？

假设，我们现在有三个监控程序，去监控一个 redis 进程；
那么，它们要怎样才能确定，这个 redis 进程到底是好好的呢？还是已经挂掉了。

要是它们三个全部给出它挂了，它才是挂了，
那是不是又回到上面说的，这是强一致性啊。

假设，三个结点，有一个结点，网络延迟，然后给出结论不一样了，那就是监控程序此刻不可用了。
也就是强一致性，再一次降低了可用性。

那么，既然如此，不能采取强一致的策略，
我们选择，让部分结点给出决策，但是，随之，又引出了新的问题，
该听谁的好？

要是有人说，主还活着，
有些人愣是认为，主已经挂了，必须赶快换一个新的主上去，这该怎么办？

先不要急着给出答案，我们来推导，
因为即使你知道答案，但是一步步发现和解决的问题你不知道，那么你的整个思维逻辑，也是不健全的。

现在，假设有 3 个结点，那么，就可以给出两种情况：
一是，一个人说了就算
二是，两个人说了才算
（三个人的话就强一致了，所以就不用考虑）

假设，现在一个人说了算的话，那么比如 A，说主死了，就要把它踢掉，
这时候，很可能其他节点都和主联系正常，主也没有任何问题，
很可能，只是这个节点，自身的连接出了问题。

那么，情况就有可能统计不准确。

此外，由于还有另外两个结点，要是它们也分别给出自己的意见，人人各执己见，那么这个集群就乱套了，
人人都能给出决策，那么就代表人人都无法给出决策。

那么，假设，现在，我们要求，必须两个结点达成一致，才能给出决定。
即使有一个结点给出不同的答案，由于另外两个保持了一致的决策，它们就可以负责，对主的存活情况给出反馈，决定是否要替换主；
而另一个结点，由于没有达到 2个 这么一个势力范围，所以，它没有权力给出意见；

因此，整个集群，给出的意见永远会是一致的。

也就是，我们的集群，给出意见，需要过半！

因为不过半的话，就会有可能不同势力范围的结点，给出不同的结论，
从而导致脑裂，
集群分区。

不过要注意一点，并不是说，脑裂，就一定不好，就一定要避免。
实际上，对于 CAP，有一个 P，叫分区容忍性，
也就是，虽然网络分区了，但是这种情况也可以接受。

比如，对于一个大片的集群，此时对于它们的监控，不一定非要用过半来保证数据结论的一致。
就算，产生了脑裂，
也就是客户访问一部分程序，得出结果是还有 2 台机器可用；另一部分程序，给出反馈 4 台机器可用；
虽然程序给出的结论产生了不一致，但是这不影响具体的使用，
因为，只要有机器存活，那么只需要选出一台机器提供服务就可以了，而不必去纠结到底有多少台还可用。

所以，对于分区的容忍性，是要看承载的数据是什么东西。
比如，上面提到的，对于一个集群有多少可用，就不一定非要数据全部一致；
但是，要是有一个 key，它的 value 是 1，还是 2，这能否允许分区，一般是不会被允许的。

所以，这还得看实际对数据的要求。

所以，对于监控主节点的过半决策，则就是避免了分区导致的结论不一致，
这里是不容许给出不一致的结论的。

![2](D:\Desktop\知识整理\图片\20200417170823725.png)

所以，我们只需要让过半给出结论即可。
但是，我再抛出一个问题，结点数量用奇数台，还是偶数台？

你可能以前没有思考过这个问题，
要是，一共有三个结点，那么就必须保证两台结点是好的，那么，整个集群才是可用的，能给出决策的。
也就是允许挂一台，
只要挂了两台，整个集群就不可用。

继续，要是整个集群有 4 台，
那么，过半就是 3 台，
也就是只允许最多挂掉 1 台，要是挂掉了 2 台，就会导致整个集群不可用。

现在，同样是只允许挂掉一台，挂掉两台就不可用了，
现在，要是总体是 3 台，那么就代表挂掉 66.7% 不可用；
要是总体是 4 台，那么就代表挂掉 50% 不可用；
4 台中，挂掉 2 台的概率会比 3 台中挂掉 2 台的概率更加大，
也就是，采用偶数台会增加风险，这时绝对不允许的。

#### 数据分片

之前提到的主从复制，也就是基于 AKF 的 x轴，它可以对主做 HA（高可用），
也可以对从节点进行读写分离，或者从不去读，仅仅做一个备机用。

但是，这没有解决一个问题，就是容量有限的问题。

一般我们的 redis 进程，我们都只会给它分配几个 G 的大小，
即便我们的一台服务器可能有上百 G 的内存，但是我们也不可能真的给我们的 redis 分配一百多 G 的内存，
因为这个持久化所消耗的时间会很大。
所以我们一般会控制我们的 redis 实例，让它只吃几个 G 的内存，这样它就会更轻盈，效率也会更高。

所以，一但数据量很大的时候，我们就需要对 redis 进行纵向集群扩容。

首先，我们可以参考上面提到的 AKF 的 y轴 做纵向业务拆分，
这样把存储的数据，在客户端层面就决定好，哪一部分的数据应该存到哪，
这样，使得每一部分的 redis 集群，只需要负责存储一部分的数据即可。

但是，这有一个问题，就是所有的业务拆分需要写在客户端的代码中，这就会造成客户端代码复杂化，
每个 redis 还是一样，它们是不用关心自己存储了哪一部分的数据。
![客户端拆分业务](D:\Desktop\知识整理\图片\20200417181553996.png)

业务拆分确实可以解决部分问题，但是，如果数据量很大，光拆分了业务之后，还是每个业务拥有大量数据；
其次，有些情况下，业务是很难拆分的，或者业务已经拆分得足够细小了，无法再进一步拆分了。

这样的情况下，就无法再对业务进行拆分了，那我们就得用其他方式，来从非业务的角度，来拆分数据。

于是就进入 sharding 分片的模式。

首先，最容易想到的就是哈希+取模，大部分程序员对哈希都是比较熟悉的。
这样，我们只需要在客户端的代码中融入哈希取模的方式，来进行存取数据，就可以实现数据的分片。

但是，这种方式最为简单，但是，问题也很显然：
就是取模的数值，是固定的，假设原本有 3 台机器，那么取模之后假设是第一台，
如果公司业务发展，数据量增大，想要增加集群数目，那么如果添加一台，那么就很可能导致，原先取模之后本来存储在第一台上的数据，现在取模之后，结果到第二台了，那么就会使得数据大片失效。

所以，这种哈希取模的方式，会影响分布式系统的扩展性。
![哈希取模](D:\Desktop\知识整理\图片\20200417183055603.png)

那我们继续探讨，假设，不用哈希取模，那么用随机的策略。
这样的话，客户端自己都不知道自己把数据塞哪了，那要怎么去取？

所以，这个方式的适用场景比较窄。
一般，只有不要求取指定单独数据的话，可以做一个消息队列的集群，
一端随机往 redis 中不断存储数据，另一端随机从 redis 中取数据，这样就不用太在意数据被存到了哪，只要最终被消费掉即可。

![消息队列](D:\Desktop\知识整理\图片\20200417183545496.png)

还有一种方式，叫做一致性哈希算法。
听名字好像和第一种很像，都有个哈希在里面，不过它的区别在于，不用对哈希值进行取模。

说白了，哈希算法，也就是映射算法，将一堆各种各样的数据，映射出一堆等宽的数据，
比如 crc16、crc32、fnv、md5 等等。

那么，我们在对这些哈希处理的时候，更倾向于作为一个环，哈希环。

![一致性哈希算法](D:\Desktop\知识整理\图片\20200417184310412.png)

我在这里不会对各种算法做详细的解释，你更多需要的，是理解这些分布式的各种解决方案。
对于这个哈希环，它需要的不仅仅是对 key 进行哈希，也需要对各个 redis 结点进行哈希运算。

比如现在，有一个大的环，上面分成了 2的32次方 个点，分别代表了 0~2的32次方-1 的所有数值，
然后，我们的所有 redis 结点，我们都可以给它一个唯一的 id 号，
这样，我们的结点，就可以经过一个固定的哈希算法，得到一个值，然后就可以映射到哈希环的某个点上。

![一致性哈希环](D:\Desktop\知识整理\图片\20200417190505296.png)

于是我们的 redis 结点，就可以分布到哈希环上。
这时，如果想要存储数据，我们也可以对数据进行哈希，得到一个值，映射到哈希环上。

这样，由于多个结点，将环分割，所以数据存储的时候，在每两个结点之间的那段数据，就归一个节点所有，又一段数据，归另一个结点所有。

![20200417191241508](D:\Desktop\知识整理\图片\20200417191241508-1626843777864.png)

为什么会搞出这么一个东西出来，那么你可以去想，现在这个哈希环上有三台结点，
假设我们现在想要增加一台，且这个节点哈希过后映射到哈希环上，恰巧是这个位置：

那么，我们会发现，现在，按照哈希映射，原本由之前的一个节点存储的数据，部分映射到了新增的结点上，
这样，就会导致，在读取数据时，会读取不到数据，导致数据丢失。
也就是，新增一个节点，不会影响前面一个节点的数据，但会导致后一个节点的部分数据丢失。

![一致性哈希环新增结点](D:\Desktop\知识整理\图片\20200417192008463.png)

不过，我们看问题也得有两面性。
为什么会出现这个哈希环，其实也是因为它的优点：
就是，在新增结点的时候，的确可以分担其他节点的压力，也就不会造成全局洗牌。
（之前哈希取模的时候，增加结点，会导致全部数据重新分配）

不过，它也不是完美的，也就是，在新增结点之后，会导致后边一个节点的部分无法被命中。

所以，如果 redis 是作为缓存，那么就会导致击穿，请求直接压到后边的 mysql 上。
那么，也可以增加解决方案，就是取数据的时候，如果前一个结点取不到数据，
就额外再到后边那一个结点取一次数据。

虽然可以使得数据又被取到，但是这样也增加了业务逻辑复杂度，也增加了网络的开销，降低了一部分效率。

其实这里还有一个小问题，就是数据在结点的分布不均匀的问题：
假设，有两个结点，但是在哈希环上的映射分布不均匀，
这样的话，大量数据都会压到一个 redis 节点上，而另一个 redis 结点，则只承担少量数据。
这样则会使资源分配不合理，一个 redis 压力大，一个 redis 资源浪费。

![20200417193829907](D:\Desktop\知识整理\图片\20200417193829907.png)

所以，这就引申出虚拟结点这么一个概念：
我们的一个 redis，不仅仅用一个点去表示它，
我们可以通过多个哈希函数，得到多个结果值，将一个节点，分布在环的多个位置上，
这样，就可以使得数据的分配，更为均衡。

![虚拟结点](D:\Desktop\知识整理\图片\20200417194258517.png)


所以，这些个方案由于这些种种问题，存在数据的丢失，所以更倾向于 redis 作为缓存的时候去使用，
而不是作为数据库去使用。

所以，技术的选择取决于业务的需求，也取决于人的判断，没有技术是一成不变的，也没有什么技术是完美无缺的。


#### 成本问题

如果我们继续探究的话，还能发现问题，

首先，无论采取什么实现，客户端都要为此融入逻辑代码来进行处理；

还有就是对于我们的 redis 结点，可能只有那么几个，但是用来连接 redis 的客户端，数量非常多，
而且，并不是一个客户端就是一个连接，我们的客户端往往会有一个连接池，一个客户端就建立了很多很多连接，需要连接的时候就直接从连接处取出一个。

所以，对于 redis，它的连接成本很高。

![连接压力](D:\Desktop\知识整理\图片\20200417195825493.png)

有此，我们可以想到，Nginx 是什么，它可以作为反向代理服务器，还有负载均衡。
也就是它自己不处理业务，不存储数据，只负责接收来自客户端的请求，把它代理到后端的服务器上。

这样，redis 服务端的连接压力就减轻了，
我们就只需要关注代理层的性能。

此外，客户端的逻辑也可以迁移，由代理层来负责实现，
这样，客户端就可以很轻盈。

其次，还可以带出来的一个词，就叫无状态。
代理层由于不需要存储数据，不牵扯到任何客户端的状态，所以本身是可以很轻易的动态增删的，
一个代理挂了，可以立刻添加上一个，
代理不够了，可以再往里面追加，所以本身是非常灵活的。
所以，一个东西，只要能做成无状态，就可以很容易的一变多。

由此，我们又可以推导出下面这个模型：

![proxy](D:\Desktop\知识整理\图片\2020041720160189.png)

要是连代理层 Nginx 都扛不住怎么办，那就可以对代理层再做一次集群，其中的每一部分客户端都只连接一个代理；
或者，我们也可以再统一接入一个负载均衡 LVS。

然后，就需要对负载做一个高可用，也就是创建一个备用机，用 keepalived 进行管理。
同时，也可以对后端的代理层监控健康状态。

这样，无论后端的技术有多么复杂，对于客户端来说都是透明的，因此，客户端的 API 就可以极其简单。
![keepalived](D:\Desktop\知识整理\图片\20200417202508944.png)

#### 数据预分区

上面探讨了三种分片算法，哈希取模，随机，一致性哈希。
看起来应该是一致性哈希最好对不对。

但是，不知道细心的你有没有发现一个问题，就是好像这种方式，都只能让 redis 在缓存的情况下才可以使用，
也就是不适用于 redis 做数据库的场景。

假设，现在有两台 redis 服务器，但是，不代表，过一年还是两台 redis 服务器。
那么，不论分片逻辑是写在客户端里，还是写在代理层里，
那么，对分片算法，都是一个挑战。

之前探讨的哈希取模，以及一致性哈希，再此时，都会因为其不足而放弃使用。

![预分区](D:\Desktop\知识整理\图片\20200417204515419.png)

那么，我们现在可以想，既然曾经是两个结点，我们要取模的话就是 %2，
那么，我们为什么不能直接在当初，就直接当成 3 个结点来进行取模分配？

所以，假设以后还会有很多次扩容，那么，这次就直接干脆点，一次性取模取 10 个，
这样，现在的两个结点，未来就可以直接增加到 10 个节点。

那这样，我们就只需要在中间加一层 mapping，让其中 0-4 这5份数据存到 1 结点，
然后让 5-9 这部分数据，存到 2 结点。

![预分配](D:\Desktop\知识整理\图片\20200417205221863.png)

这样，比如，当增加第三个结点的时候，
只需要，比如，把 1 号的 3、4 槽位以及数据分给它，把 2 号的 8、9 槽位和数据分给它，
这样，就可以继续保持每个节点，持有固定槽位的数据，而不会产生数据丢失。

![预分配](D:\Desktop\知识整理\图片\20200417210136620.png)

不过，这仍然要涉及数据迁移，可能有人会认为这样和上面所说的三种分片方式并没有本质的区别。

实际上不是的，对于之前的哈希取模，或者还是一致性哈希环，它们在新增结点的时候，都需要对数据进行重哈希，重新定位新的数据应存储在哪个结点。

而在预分配处理之后，只需要对每个 redis 加上一个哈希槽位的概念，
这样，在进行新增结点的时候，不需要对数据进行重哈希，而只是把分配过去的槽位的数据进行迁移。

有人又会问，数据迁移会不会有什么问题？
实际上，这就需要你对 redis 数据保存的知识的了解。

首先，在数据传输的过程中，肯定是先 RDB 把该传的数据全部传过去，
此外，由于服务不能停止中断，所以，可能就会有新的数据增加或修改，因此，就需要 AOF 传输少量变化的数据，
这样，它们就能将数据传输无误。

其实，redis 还有做的很好地一点，就是连代理层都不用。

在取数据的时候，客户端想连谁就连谁，
假设，客户端要 get k1，然后，连接上了 1 号 redis；
然而此时，由于 k1 取模之后的结果是 4，也就是存在了 3 号 redis 中。

那么，客户端在连接 redis 之后，redis 就需要自身去判断，这个 key 是不是应该存在我这个结点中的，
所以，redis 服务端就必须继承一个算法，对 key 进行哈希取模，
然后，也必须知晓，其它 redis 中所拥有的槽位（也就是了解整个 redis 集群中的槽位信息），

这样，1 号 redis 就会告诉客户端，这个 k1 不是存在我这里，你要去 3 号 redis 去取，
然后，客户端就可以再通过一次连接，取到正确的数据。

![获取数据](D:\Desktop\知识整理\图片\20200417211923231.png)


但是，数据分治，必然会带来一个问题，就是
聚合操作很难实现，以及事务很难实现，
比如数据要取交集等等…

虽然 redis 作者可以给你实现，但是 redis 作者没有给你实现，
这是为什么？
因为这里边有数据移动的过程。

其实 redis 作者很细腻，它一般会让计算向数据移动，而不是移动数据。

而且 redis 的最大特征就是快，所以 redis 的作者一直在做一个取舍，就是把一部分功能抹杀掉。

所以 redis 自己不去实现这个东西。
不过没关系，我们人可以实现；
所以这个锅 redis 不背，但是要由人来背。

这个锅就叫：hash tag

因为数据一但被分开，就很难再被合并处理；
换言之，如果数据不被分开，那就可以进行事务。

假设有这么两个 key：
一个是：abc-k1，一个是 abc-k2；
那么，我们需要让这两个 key 存到一起，那么就可以只对这个 abc 取模，
而不是对整个 key 取模。

这样，redis 不背这个锅，我不帮你移动数据；
但是，你也不能就把这个锅背上，起码你得把这几个 key 给放到一起去。

这样，就依然可以执行事务，因为需要在一个事务的数据都在一个 redis 里。

#### 总结

其实，你们应该也可以理解 redis 的作者，在涉及 redis 时候，他的一个出发点，
就是追求一个字：
快！

所以，我们很多时候，应该尽可能地，去利用 redis 的特性，来增强我们的系统。

而不是因为技术而技术！

你想，redis 连多线程这块领域都没有去触碰，足以说明，因为简单方而高效。

很多时候，我们过度的设计和包装，都会导致原有的高性能程序，丧失了它原有的优势。

所以更多时候，我们都会让 redis，去尽可能只是做一个缓存，而非数据库使用；
更多时候，不去追求数据的强一致性，而是允许一部分数据丢失；

所以，我们最需要的，是利用 redis 原有的优势，而不是通过技术，去强加一些 redis 本不具备的功能。

**（1）Redis Sentinal 着眼于高可用，在 master 宕机时会自动将 slave 提升为master，继续提供服务。**

**（2）Redis Cluster 着眼于扩展性，在单个 redis 内存不足时，使用 Cluster 进行分片存储。**

------

### 缓存雪崩、穿透竟该这么答？

假设，大厂面试官抛来了这个问题，你会怎么回答？

> 你如果真的只是把解决方案简单的说一遍，那肯定就当场 pass 了
> 错误示范：加锁、分散过期时间、布隆过滤器

当然，我不是说答案是错的，而是说，你不能这么去回答。

其实之前，对于缓存的击穿、雪崩、穿透，其实我已经写过一篇文章来详细阐述了。不过，我主要还是针对 Redis 这样的缓存中间件来谈及的，大部分涉及到的是技术上的难点，并且也是以一个超高并发的角度来思考、理解、以及加以阐释的。

不过，虽然文章显得很牛，不过仔细看来，你会发现，我重点也仅仅是在技术层面。（不过，你要是能把缓存击穿这个一层又一层的问题一个个抛出来然后又一个个解决，还是很不错的哈。）
那么，既然讨论到了缓存问题，涉及到了一个架构的设计思路，所以，还是得综合考虑业务的。

**脱离业务的架构，绝对不是一个好的架构！**

> 因为表面上的一个问题，它是一个问题，却又不是一个问题。
>

这话可能很迷糊：

* 当你从技术的角度去看待的时候，你会发现这是一个技术问题；
* 当你尝试去解决这个问题，你会发现可能又出现了新的问题；
* 然后当你追究问题的源头，你会发现这可能有多个问题；
* 然后当你在结合不同的业务，你会发现这些问题又变成了不同的问题；

WTF ！！！
所以当你就真的只是回答了这一个点，答了一句话，那可是要回去登通知的。

所以，上一篇文章，就可以看成，是**把一个技术点，去疯狂挖深。**
而对于缓存这块的知识，我还是决定，再结合业务，以及更多地层面，去探讨探讨其中的问题，以及解决的一些思路。
那么，你在和面试官谈的时候，就要不慌不忙，从一点一点的场景，去娓娓道来，积小流成江海；
那面试官就会觉得，你是对这些问题有想法的人了。

#### 缓存所在之处

首先，提及缓存，有些人学习了 Redis 之后，就会本能地自动将 Redis 和缓存这两者绑定起来，认为缓存就是 Redis，Redis 就是缓存，从而忽略了其他缓存的存在。

> 而实际上，我们谈及一个系统、或者说一个架构的缓存，那么，不仅仅是 Redis 这类的缓存中间件，一个系统中，涉及到缓存的地方，数不胜数。
>

比如：

* 最开始，用户的浏览器访问的时候，最前端，就是浏览器的本地缓存；
* 不仅如此，一个项目，往往会把静态资源，和用户的动态请求区分开来，将静态资源，分发到 CDN上。这样，在 CDN 这里，又做了一层缓存；
* 即使请求达到后端，在 Nginx 这里，一般，我们会本能联想到动静分离、反向代理等技术。不过，Nginx 同样可以做缓存，这又是一层缓存；
* 在往后，到了业务处理集群，我们往往才会想到我们最熟悉的缓存中间件，Redis、Memcached 等等；
  不过，不要忽视了，很多时候，由于 Redis 这样的缓存中间件还是设计网络 I/O 开销，所以，速度就不及本地缓存。所以，后端业务系统，也同样可能有本地缓存；
* 不仅如此，即使到了数据库，在数据库服务器也可能有缓存。

所以，谈到缓存，不仅仅只是 Redis 的问题，其中还涉及到很多很多方面，因为一个系统中有缓存的地方，是十分多的。

#### 缓存所带来的问题

> 那么，既然有那么多的缓存，就会涉及到和缓存有关的众多问题。

比如：

* 缓存的击穿、雪崩、穿透；
* 缓存的失效、容量、性能；
* 缓存的一致性；
* 缓存的分布式管理、集群模式、sharding 分片；
  等等等等……

所以，缓存的存在问题，是很多的。
很多时候，我们会把缓存的击穿、雪崩、穿透，分开来单独讨论，这样虽然没有什么大问题，不过，对于缓存来说，其实很多问题都是共通的。
所以很多东西都是表面，我们要探求的，就是它的本质。

> 我们去构建一个高并发系统的时候，难点在于，高并发不是由于某一个环节，而是由所有环节环环相扣的。一旦某个环节出了问题，那么系统就会出现问题。
>

而我们进行学习的时候，无论如何，都得记得 3W1H：
也就是 When / Where、What、Why、How。

而系统运行的时候，也是难免会出问题的，我们不可能说最初不管问题，碰到了问题再去解决。所以我们都往往会先**假设**，可能会出现什么问题。
我们假设会出现什么问题，那么生产中就很有可能出现什么问题（神仙莫非定律…）。所以我们可以先尝试去解决这些问题，这样，就可以尽量避免在生产环境出现问题。

所以，对于一个最常见的缓存使用场景，我们先去 Redis 中查询数据，如果能查找到，就返回结果；否则，就前往数据库查询。

那么，我们现在就需要去假想，出现什么样的情况，会使得访问缓存查询不到数据？

* 比如首先很常见的，网络不通了；
* Redis 自身也会出问题，比如说 Redis 挂了；
* 也可能是缓存过期了
* 或者 Redis 重启了；
* 也可能是内存不够用，淘汰掉了部分数据。
* 数据不存在

所以，我们查询不到缓存，其实不单单是缓存过期，它会有很多种可能性。
那么，我们就可以针对我们想到的这些可能出现的异常情况，来思考如何去处理这样的场景。

> 其实，我们的程序、架构是怎么来的，其实就是我们不断地根据底层的原理，基于一定的技术和了解，去提出各种各样的假设：
> 它会出现什么样的结果，它为什么会这样子，我为什么要去用它，如果现在出问题了怎么办，会出现什么样的问题……
> 通过不断地去问问题，去横看我们的架构，这样才能把我们的架构，打磨地越来越完美，不断地接近完美。
> 我们并没有天生就了解这个系统会出现什么问题，也无法预知，以后会出现什么问题，我们遇到的场景可能也是前人没有碰见过的。
> 但是，我们都能了解，这么一个问题的本质，它是不会变的。

就比如我们这里的 Redis，本身就是存在内存里的，如果无法读取缓存，那么问题就会是这么几个。

找出了问题之后，我们就能预料出后果。如果缓存读取不成功，就会导致大量的请求去访问数据库。
而数据库由于要磁盘操作，所以一定是慢于我们的内存的。
所以，之前我们 Redis 每秒这么多请求，没有什么问题，而给数据库之后，就会给它很大的压力。
所以，数据库就可能会崩溃；那么，相对应的以来于这个数据库的功能，也就崩溃，也就导致了整个系统崩溃；如果还有其他系统依赖这个系统，又会导致其它系统无法提供正常服务。

所以，一连串的，系统开始崩溃，也就会产生我们常说的雪崩。

> 有句话是：雪崩之下，没有一片雪花是无辜的。
>

那么，既然我们找出了可能导致雪崩的原因，也知道可能会导致的后果。
那么，我们就需要去想，这些问题，能不能去解决，或者说，怎么样能去避免。
有些问题不一定能去解决，那么我们也不会一定要去 100% 追求完美的状态，而是保持一个恰恰好的状态，针对于我们自己的业务，将其保持一个恰好的状态，问题存在，但也不会对系统造成过大的影响。

那么，下面看一下，对于这些情况，分别如何处理这些问题：

#### 处理问题

首先，可能是**网络不通**的情况。
如果出现缓存未命中的这种情况，那么，我们就有必要去关注一下错误日志，比如我们的网络不通这种情况，虽然说不是业务系统要去解决的，但是很多时候也不得不防。
因为生产环境这样的情况，是任何稀奇古怪的事情都可能发生的，比如什么网线被你一踩断了；网线被老鼠咬断了等等……或者，如果是这种发展非常迅速的公司，运维人员常常会跟不上公司发展速度，那么，服务器就很可能出各种各样的问题。

所以通常情况下，我们都会直接抛异常，记录错误日志。

第二种情况，缓存失效了，那可能是 **Redis 挂了**。
不过这种情况一般不用太担心，因为我们一般配置集群的时候，都会配置一个主从、或者主备的集群，或者是 cluster、还有哨兵这样高可用的集群。
所以一般 Redis 集群可用性还是很强的，就算有节点宕机，也不至于使 Redis 缓存不可用。

第三种情况，就是我们常说的**缓存过期**。
很多时候，我们探讨的也就是缓存过期，引发的雪崩、击穿、穿透，这样问题的技术解决方案。
不过，我们实际上是不能假设有一套方案是完美的。
因为，这类的缓存失效带来的问题，是针对于**业务系统**的。每一个公司，都有自己不同的业务系统。所以，这也就是**产生争议**的地方。

有人说，我们缓存不过期的呀！
比如说，像我们的系统配置，也就是那一些基本上不会变的、并且数据量小的，就可以设置不过期。
不过，这种不过期的终究只是少部分是吧，并且即使让它过期，也不会产生巨大的不利影响是吧。

所以，肯定**还是得有数据过期**。
那么，就会有**冷热数据分离**。

> 冷数据，就是指那部分访问频率不高的；热数据，就是那部分访问频率很高的数据。
>

比如我们的用户数据，用户每天都会用到的，就可能有这么一种情况：
有的用户，可能每天都会登录；而有的用户，则可能注册使用过一次之后，之后就再也没有使用过该业务系统。
所以，我们可以对我们的热数据，采用定期更新的方式；
而对于冷数据，则采用一定的过期时间。
也就是我们不会把所有的数据，都丢进我们的内存里面的。
你想想看，我们数据库表里有那么多数据，要是全丢给内存，内存需要多大。
我们给机器用这么大的内存，那么，这些内存，都是花昂贵的价钱买来的。
所以，我们去看这个问题的时候，不能看成是一个纯技术问题，这涉及到成本，要有成本思维。
假设，对于一个没啥用户的应用，那么很可能几个 G 内存就够用了；
如果，是对于一个高并发的应用来说，那么，就必须设置一个有效期，根据自己的业务系统，来设计过期时间是多少。

假设，如果是一个电商平台，那我们的用户信息过期时间关系不会特别大，因为电商平台，用户操作并不会那么**集中**。
如果不集中的话，可能日活一千万，可能分到每一秒，也就一百的并发而已，所以并不是很高。
只不过，用户的访问，不会那么平均，可能在早上起来上班的时候，晚上下班回家的时候，或者睡觉前，刷一刷手机，访问的用户就会比较多，而其它时间，则会更少。
所以，对于**非集中**的场景来说，缓存过期时间要求并会特别高。
不过，我们通常也需要对缓存的过期时间，做一个随机的波动。就是不要让大量的数据，在同一时间点同时过期，而是用一个随机的值，来让过期的时间分散开来，不至于太容易发生雪崩。
所以，我们可以发现，对于这类非集中的业务系统来说，设计并不会太麻烦。

但是，对于直播这种场景，就会比较复杂。
因为直播的场景，比较**集中**。
比如，一些知名人士要来直播间，那么肯定都会提前发出预告，比如 xxx 将在 x 月 x 日晚 xx 点开始直播。
那这种情况下，提前预告了什么时候直播，那么，就不只是简单地设计一个过期时间。像这类知名主播，总要来直播间直播的，那么就不用过期了，让他常驻这个系统就可以了。
所以，我们在设计的时候，还会结合到一定的数据分析，针对于一些热数据、冷数据进行分离。
在后台，就可以给这种大 v 打上一个标签，那么，可能有人直播要去看他的信息，就算不直播，也有人要去看一下他的信息，总之就是无时无刻都访问量比较大。
那么，就可以设置为不过期，或者过期时间设置得长一些，一个月，或者两三个月。

第四种情况，**内存不够用**。
很可能就是我们前期规划得就不好，内存没有分配充足。
而一般系统崩溃的原因，往往不是前面几点，而是这一点。
通常情况就是，一般互联网企业，在一开始的时候，就没想到用户量会这么大，可能一开始，大家只是琢磨着，搞了个普通的系统，可推广起来之后，用户量超出了预期。
所以，这时候即使用了缓存，那么由于前期规划不足，很多信息都需要缓存，那么，在内存管理里面，通常就会被 LRU、LFU 淘汰。
一般来说，被淘汰的数据，都是没什么人用的数据，冷门数据。但是，现如今的情况下，这种机制还是会有点问题。
因为，如今的时代，可以叫网红时代，什么都可以一夜爆红。
所以，这些东西，我们就无法预知。
前面的问题，我们可以通过一些手段来解决，但是对于这样的情况，我们就需要扩大内存。

第五种情况，**Redis 重启了**。
因为数据是存在内存里的，那么重启了，自然也就没了。
那么可以持久化，不过持久化，也不能保证数据完整地落到磁盘。
所以说，缓存失效，是一个避不开的问题。

不过，还有一种情况，导致缓存失效，是因为压根就不存在这么一条数据。
这个问题，在业界有一个专门的说法，叫做缓存穿透。
而这个问题，也是不能够去避免的。

#### 缓存穿透

在缓存失效的最后一点里，数据本身就不存在，也就是所谓的**缓存穿透**。
这就不是我们的系统问题，因为我们的系统没问题，缓存也好好的，数据库也很正常。
但是用户就是查那些本来就不存在的那些数据，比如说我们公司的**竞争对手**。
这种事情很常见，尤其是一些初创型的企业，再常见不过了，连美团支付宝，线下也都要打架。
所以，如果有人去查询这样必然不存在的数据，而你的系统，恰好就没有对应的机制，那么，这时候，你的系统，可能就被攻击透了。
而且，这些请求，又恰好都是正常的请求，也没有什么不对的地方，就算发生了，你也可能感觉不出来，你可能就是发现，这个系统挂了。

所以，我们往往要对重要的接口，进行**查询校验**。
所以，参数不是乱来的，这一块就已经算是涉及到了防范攻击了。
怎么防止他乱填，那就要参数校验，比如，搞一些特殊的生成方式。
可能用户看到的 id，就是一串 1234567 的数字，但是这个数字，可能就不是随便写的，它是会被赋予一种方式，计算得出的。
比如说，id 的第一位必须是奇数；比如说，第三位数字，必须是 3……等等等等
也就是，我们的 id 可以是一串数字，但是，它不是一个纯粹的从 1 开始递增的一个数字，而是有特殊意义的数字，我们可以不通过查数据库查缓存发现它不存在，而是只通过一个简单地校验，就把它校验出来。而这个规则，一般人也很难去猜到。
不过，这么做有效，单也不一定能全部防范。
不管怎么说，这些东西，都是根据你的业务规则，去进行调整的。

第二种方式，就比较直接。
我如果在数据库里也查询不到，那么，我就直接在缓存里放入一个值表示该记录不存在。
这样的话，我们就可以把有效期设置得比较短。
因为，这个数据虽然现在不存在，但是不代表未来也不存在，因为能被查询，说明那肯定是符合我们的参数校验规则的。
所以，这种方案，就是**临时缓存表示空值**。

如果，这些问题都解决不了。
那么，就需要在整个系统中，校验一下，整个系统中是否存在这个数据。
简单地做法，就是把所有的 id，存在 Redis 里面，每次查询时，先看看 Redis 中有没有这个 id，有的话，就代表数据存在。
这个做法对很多数据量不是很大的系统来说，也是完全 OK 的。
不过，有一个问题就是，对于那些量很大的应用系统，把 id 全部放入内存是不现实的。
假设，一个 id 用 long 来表示，那么一个 id 就是 8Byte，那么10E 的商品，就是大约 8G 的内存空间。
而且，除了商品 id，我们系统肯定还会有用户 id，订单 id 等等……
所以，光是存这些 id，我们就会耗费大量的内存，资源的消耗就过于多了。
不过对于一些小系统，少掉几个数量级，变成 M、K 的数量级，倒也是很合适的。所以，我们往往也不用去死记硬背，或是生搬硬套，非要整出一个布隆过滤器上去。

只有碰到真正这样的海量数据，我们就得思考，怎么去解决。
在业界就有一个算法，既然要减少内存占用，那就可以用**布隆过滤器**，它来自于 1970 年，布隆提出来。
它就有点类似 HashMap 这样的做法，根据 key 取一个 hash，然后，定位一个 index，也就是下标，然后再判断，这个下标有没有数据，没有，就代表空。
布隆过滤器就类似如此，也是用一个数组，只不过，这个数组，不存放 id，因为这个 id 太占空间了。
它是一个 bit 数组，每一个下标，对应一个 bit，只能存 0 或者 1。
所以，这个数组很长，但是，占用的空间，却又非常非常小。

不过，布隆过滤器虽然看起来很优秀，非常节省空间，
但是，布隆过滤器是由缺点的：
因为**哈希碰撞是无法避免的**，所以，就存在那些不存在的 id，它们对应的数组下标对应到一个存在 id 的下标，那么就会使得博隆过滤器误判该数据存在。

所以，布隆过滤器，为了尽力减少冲突，布隆过滤器一般采取多重哈希，只有所有哈希值都满足，才判定为存在。

![在这里插入图片描述](D:\Desktop\知识整理\图片\20200814000840907.png)

不过，其实有些时候，这些也完全都是多余的：
如果，你的 id 都是自增 id 的话，那么，是不是只要判断一下，有没有超过最大 id 就好了？
没有超过最大 id 的，就是正常请求，超过最大 id 的，就是非法请求。

> 所以，实际上，采取什么措施，都是根据业务系统去进行考量和取舍的。
>

不过互联网企业正常不采用自增 id，不然的话，很容易就给竞争对手猜出业务量。
在内部使用的话，就可以考虑用自增 id 了。

#### 限流

那么，我们通过上面的方式，发现，有了各种各样不同的方案，去应对我们的缓存失效，来保证服务的良好运行；
但是，终究**无法**做到 100%。
也就是一定会**有漏网之鱼**！

所以说，缓存失效，我们可以有效控制，但做不到万无一失。
比如网络的不稳定，数据的过期，LRU 清除，以及节点的宕机，重启，布隆过滤器的误判……等等等等

> 那么，首先，不管你遇到的场景是什么，首先，只要你是高并发场景，那么就要记住这四个字：
> **限流、分流**

只要出现缓存雪崩，那么最大的出问题的地方，就是我们的数据库。
所以，我们的问题就可以转化一下：
数据库出问题了，我们该怎么办？
或者说如果要让数据库不出问题，我们该怎么办？

既然是因为请求过多导致数据库扛不住，那么，我们就要不让数据库，接受那么多的请求。
所以，我们可以做的，就是**限流**。
而限流的话，只是个概念，我们究竟该怎么去做？我们一般都有很多小的措施。
典型的限流方法，就是**信号量限流**。
在 Java 的并发包里，就有 Semaphore 这个并发工具类。
每次访问数据库，都要获取这个信号量，比如我们设置 100，那么前 100 个请求还在访问数据库时，第 101 个请求就会阻塞，就会因为获取不到信号量而暂时无法访问数据库，这样，就对数据库进行了一个限流。

不过，有一个很有意思的问题，为什么要在数据库操作之前加一个 Semaphore？
你可能要问，我不是刚刚说了，要限流吗？
不知道你们有没有想过这样一个问题，我们假设，缓存失效的情况下，所有请求，会打到我们的数据库。
**不过，所有的请求，会打到数据库吗？**
。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。。

你不要被这个轻易地迷惑了。
我们数据库这块，是不是有一个数据库连接池？
**所以，到了数据库连接池这里，大量的请求，连数据库连接都获取不到，怎么去怼我们的数据库？**
**那么，既然有数据库连接池，那么还为什么要信号量？压根不会出现数据库被怼炸的情况啊？**

所以，实际上，用信号量，实际是一个比较偷懒的方法。
在很多用户同时查数据的时候，可能一堆用户查询的是主播 A，一堆用户查询的是 B，还有一堆查询 C，还有 D。
那么，就可不可能有这么一种情况，Semaphore 信号量，被查询 A 的用户全部抢走，那么剩下的 B、C、D 的用户是不是就不爽，也就是其它主播的粉丝很不爽。
所以这种情况下是不太友好的。
那么我们可以针对不同的主播，分别设置一个 Semaphore 信号量，那么，在用户访问的时候，不同主播的粉丝，就不会出现等待差异过大的问题了。

那么，再回到这么一个问题，为什么在一个系统里有数据库连接池还需要 Semaphore。
因为，一个系统中的数据库**连接池是给系统中的整个应用使用的**，而比如查询主播，可能只不过是**众多接口中的一个**罢了。
那么，假设我们不加任何限制，完全利用数据库连接池的机制来限流。那么，假设，这一个查询主播的功能，就用光了连接池里的所有连接，那么，其它功能还玩啥？
所以说，再加一个 Semaphore 信号量，是为了再做一个隔离。
而这一个功能，只允许用一部分的资源，剩下的资源，还需要留给其它功能去使用。

> 我们经常和限流划在一起的，是**降级**。
>

虽然说，限流可以抑制住大量请求，但是，Semaphore 信号量会使得大量请求阻塞等待。用户那里，就会一直转圈圈，卡在那访问不到，但是又还在停在那等着出结果。所以用户体验就会特别差。
比如双十一的时候，系统用不了会怎么样，是不是会显示：失败请重试；查询状态出现错误；网络不给力……等等反馈。
不过，降级的话，是随随便便就可以降得吗？
尤其是支付宝、微信等等应用，你能看到的降级页面，这些语句、词语，都是通过重重审核下来的。
要是支付宝天天都给你弹出错误，那这个也说不过去了。
也就是系统得有一定的**容错**机制，比如重试一下，而不是一碰到问题就给错误，就降级。
所以，降级常常也会和一个词连在一起，就是容错，就叫做：
**容错降级**。

#### 总结

> 其实大家很多时候，都只是把缓存雪崩、穿透之类的，单独看成一个问题，意图是找到一个完美的方法。
> 而我这里提到的，也不仅仅只是一个缓存的过期，而是一系列缓存可能失效的原因，这些往往是大家会疏忽的地方。
> 很少有人说，可以把这些问题联系起来，把问题综合起来，能结合不同的场景去看待的。
> 而现实中也往往没有一个完美的方法，能完整地解决这样的问题，因为在不同的场景中，问题往往又会有不同的变化。
> 所以，要想成为一名优秀的架构师，那思维就一定不能有局限，你能想到别人想不到的问题，那你才能比别人多一份机会。









